# Default configuration for the LLM From Scratch project

# Model parameters
model:
  model_type: 'gpt'
  vocab_size: 30000
  d_model: 256
  num_heads: 4
  num_layers: 4
  d_ff: 1024
  max_seq_length: 256
  dropout: 0.1

# Tokenizer parameters
tokenizer:
  tokenizer_type: 'bpe'
  tokenizer_path: 'tokenizers/bpe_wikitext_large.json'

# Data parameters
data:
  dataset: 'wikitext' # 'wikitext' or 'text_files'
  data_dir: 'data'
  text_files_dir: null # Specify path if using 'text_files'

# Training parameters
training:
  batch_size: 16
  learning_rate: 0.0001 # 3e-4
  max_epochs: 10
  checkpoint_dir: 'checkpoints/wikitext_model_large'
  device: "cpu"
  device: null # 'cuda', 'cpu', or null for auto-detection
  resume: null # Path to a checkpoint to resume from

# Generation parameters
generation:
  generate: false # Set to true to run in generation mode
  prompt: 'Once upon a time'
  max_length: 100
  temperature: 1.0 